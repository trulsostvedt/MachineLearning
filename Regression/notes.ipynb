{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133027b5",
   "metadata": {},
   "source": [
    "## Notes to the report (not part of delivery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b16d33",
   "metadata": {},
   "source": [
    "Jeg forsøkte først kun én modell, koden til denne finnes under. \n",
    "\n",
    "Som jeg har kommentert i koden så prøvde jeg først polynomgrad 2–6, 4 og 5 ga nesten likt resultat på CV R^2 (0.93 ish), så 4 er nok bedre da den er lavere risiko for overfitting, og bruker heller det i det mymodel, men verdt å kommentere i rapporten for å vise refleksjon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9679cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_regression.py\n",
    "# -------------------\n",
    "# 1) Leser treningsdata\n",
    "# 2) Gjør modellvalg med GridSearchCV (PolynomialFeatures + StandardScaler + Ridge)\n",
    "# 3) Trener beste modell på hele datasettet\n",
    "# 4) Lagrer modellen (best_model.pkl) + en liten JSON med beste hyperparametre\n",
    "\n",
    "import json, joblib, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "\n",
    "# Data \n",
    "base = Path(__file__).resolve().parent  # path to this script\n",
    "X = np.load(base / \"X_train.npy\")   # shape (700, 6)\n",
    "y = np.load(base / \"Y_train.npy\")   # shape (700,)\n",
    "\n",
    "# Pipeline og hyperparametre \n",
    "pipe = Pipeline([\n",
    "    (\"poly\",   PolynomialFeatures(include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\",  Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"poly__degree\": [2, 3, 4], #[2, 3, 4, 5, 6],  prøvde polynomgrad 2–6, både 4 og 5 ga nesten likt resultat på CV R^2, så 4 er nok bedre da den er lavere risiko for overfitting\n",
    "    \"ridge__alpha\": [1e-4 ,1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000],  # regulariseringsstyrke\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(pipe, param_grid, cv=cv, scoring=\"r2\", refit=True, n_jobs=-1)\n",
    "\n",
    "print(\"Running GridSearchCV...\")\n",
    "gs.fit(X, y)\n",
    "print(\"Best params:\", gs.best_params_, \" | CV R^2:\", gs.best_score_)\n",
    "\n",
    "# Trener beste modell på hele datasettet ---\n",
    "best_model = gs.best_estimator_\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Lagrer modell og metadata ---\n",
    "joblib.dump(best_model, base / \"best_model.pkl\")\n",
    "with open(base / \"best_model_info.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"best_cv_r2\": gs.best_score_,\n",
    "        \"model_family\": \"PolynomialFeatures + StandardScaler + Ridge\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Lagret: best_model.pkl og best_model_info.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599bfeac",
   "metadata": {},
   "source": [
    "Videre la jeg til to andre modeller, og sammenlignet. Den nye koden tar nå å sjekker de 3 modellene opp mot hverandre, og lagrer den beste til best_model.pkl. Som videre loades av mymodel.py. Ny kode finnes under: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d570950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests 3 model families (Poly+Ridge, Poly+Lasso, KernelRidge RBF).\n",
    "# Saves best_model.pkl (the winner), best_model_info.json (structured),\n",
    "# and cv_results.txt (I added this log to use when writing the report, not necessarily for the model).\n",
    "\n",
    "import json, joblib, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# --- Load data ---\n",
    "base = Path(__file__).resolve().parent\n",
    "X = np.load(base / \"X_train.npy\")\n",
    "y = np.load(base / \"Y_train.npy\")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "# 42 = the answer to life, the universe, and everything - ref to Hitchhiker's Guide to the Galaxy our inside joke\n",
    "# when choosing random states :) its \"random\" but fixed so results are reproducible\n",
    "results = {}\n",
    "\n",
    "def summarize_gs(gs):\n",
    "    \"\"\"Pick out best score/params + std for the best row.\"\"\"\n",
    "    idx = gs.best_index_\n",
    "    mean = float(gs.cv_results_[\"mean_test_score\"][idx])\n",
    "    std  = float(gs.cv_results_[\"std_test_score\"][idx])\n",
    "    return {\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"cv_r2_mean\": mean,\n",
    "        \"cv_r2_std\": std,\n",
    "    }\n",
    "\n",
    "# Model 1: Poly+Ridge \n",
    "pipe_ridge = Pipeline([\n",
    "    (\"poly\",   PolynomialFeatures(include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\",  Ridge())\n",
    "])\n",
    "grid_ridge = {\n",
    "    \"poly__degree\": [2, 3, 4], # 4 and 5 were very close, but 4 is likely better to avoid overfitting\n",
    "    \"ridge__alpha\": [1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "}\n",
    "gs_ridge = GridSearchCV(pipe_ridge, grid_ridge, cv=cv, scoring=\"r2\", refit=True, n_jobs=-1)\n",
    "gs_ridge.fit(X, y)\n",
    "results[\"poly_ridge\"] = summarize_gs(gs_ridge)\n",
    "\n",
    "# Model 2: Poly+Lasso \n",
    "pipe_lasso = Pipeline([\n",
    "    (\"poly\",   PolynomialFeatures(include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\",  Lasso(max_iter=50000, tol=1e-3, selection=\"cyclic\")) # Max iter increased to ensure convergence (It didn't converge with default 1000)\n",
    "])\n",
    "grid_lasso = {\n",
    "    \"poly__degree\": [2, 3, 4], # 4 and 5 were very close, but 4 is likely better to avoid overfitting (see notes.ipynb)\n",
    "    \"lasso__alpha\": [1e-3, 1e-2, 1e-1, 1, 10, 100], \n",
    "}\n",
    "gs_lasso = GridSearchCV(pipe_lasso, grid_lasso, cv=cv, scoring=\"r2\", refit=True, n_jobs=-1)\n",
    "gs_lasso.fit(X, y)\n",
    "results[\"poly_lasso\"] = summarize_gs(gs_lasso)\n",
    "\n",
    "# Model 3: Kernel Ridge (RBF)\n",
    "pipe_kr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"kr\",     KernelRidge(kernel=\"rbf\"))\n",
    "])\n",
    "grid_kr = {\n",
    "    \"kr__alpha\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"kr__gamma\": [1e-3, 1e-2, 1e-1, 1.0],\n",
    "}\n",
    "gs_kr = GridSearchCV(pipe_kr, grid_kr, cv=cv, scoring=\"r2\", refit=True, n_jobs=-1)\n",
    "gs_kr.fit(X, y)\n",
    "results[\"kernel_ridge_rbf\"] = summarize_gs(gs_kr)\n",
    "\n",
    "# We select the best model based on CV R^2\n",
    "best_name = max(results.keys(), key=lambda k: results[k][\"cv_r2_mean\"])\n",
    "best_est  = {\n",
    "    \"poly_ridge\": gs_ridge.best_estimator_,\n",
    "    \"poly_lasso\": gs_lasso.best_estimator_,\n",
    "    \"kernel_ridge_rbf\": gs_kr.best_estimator_,\n",
    "}[best_name]\n",
    "\n",
    "best_est.fit(X, y)\n",
    "joblib.dump(best_est, base / \"best_model.pkl\")\n",
    "\n",
    "# JSON log; source of inspiration: https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file\n",
    "with open(base / \"best_model_info.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"winner\": best_name,\n",
    "        \"results\": results,\n",
    "        \"notes\": \"cv_r2_mean ± cv_r2_std with 5-fold KFold(shuffle, rs=42). \"\n",
    "                 \"best_model.pkl is trained on all 700.\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Text log for easy reading when we are writing the report\n",
    "log_path = base / \"cv_results.txt\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"CV Results (5-fold, R^2)\\n\")\n",
    "    for k, info in results.items():\n",
    "        mean = info[\"cv_r2_mean\"]; std = info[\"cv_r2_std\"]\n",
    "        f.write(f\"{k:>18s}: {mean:.4f} ± {std:.4f} | params: {info['best_params']}\\n\")\n",
    "    f.write(f\"\\nWinner: {best_name}\\n\")\n",
    "\n",
    "print(f\"\\nWinner: {best_name} --> saved to best_model.pkl\")\n",
    "print(f\"Details saved to best_model_info.json and cv_results.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008a223",
   "metadata": {},
   "source": [
    "### Modell-sammenligning (5-fold CV, R²)\n",
    "\n",
    "Nedenfor er resultatene fra GridSearchCV (KFold=5, shuffle=True, rs=42). Jeg sammenlignet tre modeller:  \n",
    "- **Poly + Ridge** (polynom-features + standardisering + Ridge)  \n",
    "- **Poly + Lasso** (polynom-features + standardisering + Lasso)  \n",
    "- **Kernel Ridge (RBF)** (standardisering + RBF-kjerne)\n",
    "\n",
    "| Modell                 | Hyperparametre (beste)                          | R² (mean ± std)   |\n",
    "|-----------------------|--------------------------------------------------|-------------------|\n",
    "| Poly + Ridge          | degree = 4, alpha = 10                           | **0.9331 ± 0.0142** |\n",
    "| Poly + Lasso          | degree = 4, alpha = 0.01                         | **0.9414 ± 0.0067** |\n",
    "| Kernel Ridge (RBF)    | alpha = 0.01, gamma = 0.1                        | **0.9829 ± 0.0028** |\n",
    "\n",
    "**Observasjoner / tolkning**\n",
    "\n",
    "- Kernel Ridge (RBF) skårer klart høyest på CV-R² og har også lav varians mellom foldene. Det tyder på at RBF-kjernen fanger opp en mer kompleks, men relativt “glatt” ikke-lineær sammenheng i dataene.  \n",
    "- Poly + Ridge og Poly + Lasso ligger lavere, men fortsatt høyt (≈0.93–0.94). Lasso gjør det litt bedre enn Ridge her, trolig fordi ℓ1-regularisering straffer og nuller ut noen koeffisienter når antall polynomledd blir stort.  \n",
    "- Grad 4 ga best balanse for polynom-modellene i mitt oppsett. Jeg testet også høyere grader og så marginale forbedringer, men med økt kompleksitet.  \n",
    "- På bakgrunn av CV-resultatene valgte jeg **Kernel Ridge (RBF)** som endelig modell til innleveringen. (Den trenes så på alle 700 før lagring som `best_model.pkl`.)\n",
    "\n",
    "*Tekniske detaljer:*  \n",
    "- CV-oppsett: KFold(n_splits=5, shuffle=True, random_state=42), `scoring=\"r2\"`.  \n",
    "- Alle modellene kjøres i `Pipeline` for å unngå datalekkasje (skalering og feature-laging fit’es kun på treningsfold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6937557",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"winner\": \"kernel_ridge_rbf\",\n",
    "  \"results\": {\n",
    "    \"poly_ridge\": {\n",
    "      \"best_params\": {\n",
    "        \"poly__degree\": 4,\n",
    "        \"ridge__alpha\": 10\n",
    "      },\n",
    "      \"cv_r2_mean\": 0.933108596320227,\n",
    "      \"cv_r2_std\": 0.014155560800823133\n",
    "    },\n",
    "    \"poly_lasso\": {\n",
    "      \"best_params\": {\n",
    "        \"lasso__alpha\": 0.01,\n",
    "        \"poly__degree\": 4\n",
    "      },\n",
    "      \"cv_r2_mean\": 0.9413730769574187,\n",
    "      \"cv_r2_std\": 0.0067469533124386045\n",
    "    },\n",
    "    \"kernel_ridge_rbf\": {\n",
    "      \"best_params\": {\n",
    "        \"kr__alpha\": 0.01,\n",
    "        \"kr__gamma\": 0.1\n",
    "      },\n",
    "      \"cv_r2_mean\": 0.9828514788918407,\n",
    "      \"cv_r2_std\": 0.002842091276182927\n",
    "    }\n",
    "  },\n",
    "  \"notes\": \"cv_r2_mean \\u00b1 cv_r2_std with 5-fold KFold(shuffle, rs=42). best_model.pkl is trained on all 700.\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
